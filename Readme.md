Efficient Memory Management for Large Language Model Serving with PagedAttention [VLLM]
https://arxiv.org/abs/2309.06180

Attention Is All You Need [Attention paper]
https://arxiv.org/abs/1706.03762

flash attention
https://arxiv.org/abs/2205.14135

FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
https://arxiv.org/abs/2307.08691

Flux
https://i.redd.it/7n3ix8do9vgd1.png

LLama 3.1 Herds of models
https://ai.meta.com/research/publications/the-llama-3-herd-of-models/

ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT
https://arxiv.org/abs/2004.12832

Solving Quantitative Reasoning Problems with Language Models [Minerva paper]
https://arxiv.org/abs/2206.14858
https://youtu.be/jksGQhMtXjo?t=1981

Instruction Tuning for Large Language Models: A Survey 
https://arxiv.org/pdf/2308.10792

Yi: Open Foundation Models by 01.AI
https://arxiv.org/abs/2403.04652

Large Language Models: A Survey
https://arxiv.org/abs/2402.06196

GPT-4 Technical Report
https://arxiv.org/abs/2303.08774

Mamba: Linear-Time Sequence Modeling with Selective State Spaces
https://arxiv.org/pdf/2312.00752

Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality
https://arxiv.org/abs/2405.21060

Jamba: A Hybrid Transformer-Mamba Language Model
https://arxiv.org/pdf/2403.19887
