# Research Papers

A collection of research papers related to large language models and attention mechanisms.

## Table of Contents

- Efficient Memory Management for Large Language Model Serving with PagedAttention
- Attention Is All You Need
- FlashAttention
- FlashAttention-2
- Flux
- LLama 3.1 Herds of Models
- ColBERT
- Solving Quantitative Reasoning Problems with Language Models
- Instruction Tuning for Large Language Models: A Survey
- Yi: Open Foundation Models by 01.AI
- Large Language Models: A Survey
- GPT-4 Technical Report
- Mamba
- Transformers are SSMs
- Jamba

## Papers

| Title                                                                 | Link                                                                 |
| --------------------------------------------------------------------- | -------------------------------------------------------------------- |
| Efficient Memory Management for Large Language Model Serving with PagedAttention [VLLM] | arXiv:2309.06180                 |
| Attention Is All You Need [Attention paper]                           | arXiv:1706.03762                 |
| FlashAttention                                                        | arXiv:2205.14135                 |
| FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning | arXiv:2307.08691                 |
| Flux                                                                  | Image                         |
| LLama 3.1 Herds of models                                             | Meta AI |
| ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT | arXiv:2004.12832                 |
| Solving Quantitative Reasoning Problems with Language Models [Minerva paper] | arXiv:2206.14858 YouTube |
| Instruction Tuning for Large Language Models: A Survey                | arXiv:2308.10792                 |
| Yi: Open Foundation Models by 01.AI                                   | arXiv:2403.04652                 |
| Large Language Models: A Survey                                       | arXiv:2402.06196                 |
| GPT-4 Technical Report                                                | arXiv:2303.08774                 |
| Mamba: Linear-Time Sequence Modeling with Selective State Spaces      | arXiv:2312.00752                 |
| Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality | arXiv:2405.21060                 |
| Jamba: A Hybrid Transformer-Mamba Language Model                      | arXiv:2403.19887                 |

